# -*- coding: utf-8 -*-
"""notebook-sistem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14b-w5L4Wz-FOr3VrNYDg7IvE86jHRfYc

# **Recommendation System**

## Deskripsi Proyek

Perkembangan teknologi digital telah membawa dampak positif bagi industri hiburan, terutama dalam hal ketersediaan konten film yang melimpah. Namun, hal ini juga menyebabkan pengguna mengalami kesulitan dalam memilih film yang tepat. Dalam konteks ini, sistem rekomendasi film berperan penting sebagai solusi untuk membantu pengguna menavigasi berbagai pilihan dan menemukan film yang paling relevan dengan preferensi mereka.

Dataset diambil dari [https://grouplens.org/datasets/movielens/100k/]

## Import Library
"""





import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import train_test_split, KFold
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error, mean_absolute_error
from scipy.sparse import coo_matrix, csr_matrix
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
from math import sqrt
import implicit

"""## Data Understanding

### Gathering Data

Membaca data & melihat preview data
"""

links = pd.read_csv('links.csv')
movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')
tags = pd.read_csv('tags.csv')
print(links.head(), links.shape, movies.head(), movies.shape, ratings.head(), ratings.shape, tags.head(), tags.shape)

"""### Assessing Data

#### Checking Missing Value

Mengecek nilai kosong(null) di masing masing data
"""

print(links.isnull().sum(), movies.isnull().sum(), ratings.isnull().sum(), tags.isnull().sum())

"""#### Checking Duplicate Data

Mengecek nilai duplikat di masing masing dataset
"""

print(links.duplicated().sum(), movies.duplicated().sum(), ratings.duplicated().sum(), tags.duplicated().sum())

"""### Exploratory Data Analysis (EDA)

#### Unique Value

##### Mengecek nilai unique untuk userid, movieid
"""

print(movies.movieId.nunique(), ratings.userId.nunique(), tags.userId.nunique(),
         tags.tag.nunique(), movies.genres.nunique())

"""##### Mengecek nilai unique genre"""

genres_split = movies.genres.str.split('|', expand=True)
genres_split.stack().unique()

"""##### Mengecek value counts tiap genre"""

genres_split = movies.genres.str.split('|').explode()
genre_counts = genres_split.value_counts()
genre_counts

"""#### Wordcloud

##### Inisiasi fungsi wordcloud
"""

def wordcloud(data, title):
    wc = WordCloud(width=800, height=400, max_words=200, background_color='white').generate(' '.join(data))
    plt.figure(figsize=(10, 8))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()

"""##### Wordcloud untuk Genre"""

wordcloud(genres_split, 'Genres WordCloud')

wordcloud(movies.genres, 'Genres WordCloud')

"""##### Wordcloud tags"""

wordcloud(tags.tag, 'Tags WordCloud')

"""#### Film berdasarkan Rating"""

print(movies.head(), ratings.head(), tags.head())

"""##### Menggabung data movies dan ratings"""

movies_ratings = pd.merge(movies, ratings, on='movieId')
print(movies_ratings.head(), movies_ratings.shape)

"""#### Rata rata rating tertinggi film

"""

movies_ratings.groupby('title')['rating'].mean().sort_values(ascending=False).head(10)

"""#### Rata rata rating terendah film"""

movies_ratings.groupby('title')['rating'].mean().sort_values(ascending=True).head(10)

"""#### Film yang paling banyak diberi rating"""

movies_ratings.groupby('title')['rating'].count().sort_values(ascending=False).head(10).plot(kind='bar')

"""## Data Preparation

### General Data Preparation

Mengubah tipe data kolom timestamp menjadi datetime
"""

tags.timestamp = pd.to_datetime(tags.timestamp, unit='s')
ratings.timestamp = pd.to_datetime(ratings.timestamp, unit='s')
print(tags.head(), ratings.head())

"""Menggabung data movies dengan links"""

movies_full = pd.merge(movies, links, on='movieId')
print(movies_full.head(), movies_full.shape, movies_full.isnull().sum())

"""Mengecek baris yang terdapat nilai kosong (null)"""

links[links.tmdbId.isnull()]

"""Membuang baris yang terdapat nilai kosong (null)"""

links.dropna(inplace=True)
print(links.head(),links.shape, links.isnull().sum())

"""Membersihkan tahun rilis judul film  """

movies['title'] = movies['title'].str.replace(r'\(\d{4}\)', '',
                                                        regex=True).str.strip()
movies['title'] = movies['title'].str.replace(r'\(\d{4}(â€“\d{4})?\)', '',
                                                        regex=True).str.strip()
print(movies, movies.isnull().sum())

"""Membersihkan kolom genres dengan menghapus karakter seperti [] dan tanda |"""

tfidf = TfidfVectorizer(stop_words='english')

movies['genres'] = movies['genres'].str.replace(r"[\[\]']", "", regex=True).str.replace("|", " ")

movies['features'] = movies['title'] + " " + movies['genres']
movies

"""### Data Preparation untuk Modelling Content Based Filtering

Pada bagian ini saya mengubah fitur judul film menjadi matriks TF-IDF, lalu menghitung kesamaan antarfilm menggunakan cosine similarity untuk mendapatkan nilai kemiripan setiap film satu sama lain.
"""

tfidf_matrix = tfidf.fit_transform(movies['features'])
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
cosine_sim

"""### Data Preparation untuk Modelling Collaborative Filtering

Membagi data menjadi train dan test, lalu menghitung RMSE dan MAE untuk evaluasi akurasi model.
"""

# Konversi DataFrame menjadi sparse matrix
user_ids = ratings['userId'].astype('category').cat.codes.values
movie_ids = ratings['movieId'].astype('category').cat.codes.values
ratings_values = ratings['rating'].values

sparse_ratings = coo_matrix((ratings_values, (user_ids, movie_ids)))

# Split data (pertahankan struktur sparse)
train, test = train_test_split(sparse_ratings, test_size=0.2, random_state=42)

# Konversi ke CSR format (diperoleh oleh implicit)
train_csr = train.tocsr()
test_csr = test.tocsr()

# Inisialisasi model Alternating Least Squares (ALS)
model = implicit.als.AlternatingLeastSquares(
    factors=50,
    iterations=20,
    random_state=42
)

# Train model (gunakan confidence untuk explicit feedback)
model.fit(train_csr * 2)  # Skala rating sebagai confidence

# Rekomendasi
user_vecs = model.user_factors
item_vecs = model.item_factors

"""# Model Development

## Content Based Filtering

Membuat fungsi untuk merekomendasikan 10 film mirip berdasarkan kesamaan judul yang diberikan, menggunakan cosine similarity.
"""

# Fungsi untuk rekomendasi film (Content Based Filtering menggunakan Cosine Similarity)
def recommend_movies(title, cosine_sim=cosine_sim):

    idx = movies[movies['title'] == title].index[0]

    sim_scores = list(enumerate(cosine_sim[idx]))

    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    sim_scores = sim_scores[1:11]

    movie_indices = [i[0] for i in sim_scores]

    return movies.iloc[movie_indices][['movieId','title', 'genres']]

"""Kode ini akan menampilkan 10 rekomendasi film yang mirip dengan "Jumanji" berdasarkan kesamaan konten film."""

recommend_movies("Jumanji")

"""Kode ini akan menampilkan 10 rekomendasi film yang mirip dengan "Waiting to Exhale" berdasarkan kesamaan konten film."""

recommend_movies("Waiting to Exhale")

"""Kode ini akan menampilkan 10 rekomendasi film yang mirip dengan "Toy Story" berdasarkan kesamaan konten film."""

recommend_movies("Toy Story")

"""Fungsi-fungsi untuk menghitung metrik evaluasi untuk sistem rekomendasi"""

# Fungsi evaluasi baru untuk Content-Based Filtering
def precision_recall_content_based(input_movie, recommended_movies, k=10):
    input_tfidf = tfidf.transform([input_movie])
    rec_tfidf = tfidf.transform(recommended_movies)

    # Menghitung cosine similarity antara input dan rekomendasi
    sim_scores = cosine_similarity(input_tfidf, rec_tfidf)[0]

    # Menghitung precision dan recall berdasarkan similarity scores
    precision = sum(sim_scores[:k]) / k
    recall = sum(sim_scores[:k]) / sum(sim_scores) if sum(sim_scores) > 0 else 0

    # Mengembalikan nilai precision dan recall dalam persentase
    return precision * 100, recall * 100

# Contoh penggunaan evaluasi untuk dua film input
input_movies = ["Toy Story", "Jumanji"]
recommendation_results = {movie: list(recommend_movies(movie)['title']) for movie in input_movies}

for movie in input_movies:
    recommended_titles = recommendation_results[movie]
    precision, recall = precision_recall_content_based(movie, recommended_titles, k=5)
    print(f"Precision@5 for '{movie}': {precision:.2f}%")
    print(f"Recall@5 for '{movie}': {recall:.2f}%")

"""## Collaborative Filtering"""

# Untuk model berbasis embedding (lebih mirip SVD)
# Membuat pivot table
ratings_pivot = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)

# Untuk model berbasis neighborhood
model = NearestNeighbors(metric='cosine', algorithm='brute')
# Menggunakan ratings_pivot yang sudah didefinisikan sebelumnya
model.fit(ratings_pivot.T)

"""Melatih model SVD untuk rekomendasi film"""

# Fungsi untuk menghitung RMSE
def custom_rmse(y_true, y_pred):
    return sqrt(mean_squared_error(y_true, y_pred))

# Fungsi untuk menghitung MAE
def custom_mae(y_true, y_pred):
    return mean_absolute_error(y_true, y_pred)

# Membuat pivot table
ratings_pivot = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)

# Train-test split
X_train, X_test = train_test_split(ratings_pivot, test_size=0.2, random_state=42)

# Inisialisasi dan train model SVD
svd = TruncatedSVD(n_components=20, random_state=42)
svd.fit(X_train)

# Prediksi
train_preds = svd.inverse_transform(svd.transform(X_train))
test_preds = svd.inverse_transform(svd.transform(X_test))

# Evaluasi
train_rmse = custom_rmse(X_train.values.flatten(), train_preds.flatten())
test_rmse = custom_rmse(X_test.values.flatten(), test_preds.flatten())

train_mae = custom_mae(X_train.values.flatten(), train_preds.flatten())
test_mae = custom_mae(X_test.values.flatten(), test_preds.flatten())

print(f"Train RMSE: {train_rmse:.4f}")
print(f"Test RMSE: {test_rmse:.4f}")
print(f"Train MAE: {train_mae:.4f}")
print(f"Test MAE: {test_mae:.4f}")

# Cross-validation manual

kf = KFold(n_splits=5)
rmse_scores = []
mae_scores = []

for train_index, test_index in kf.split(ratings_pivot):
    X_train = ratings_pivot.iloc[train_index]
    X_test = ratings_pivot.iloc[test_index]

    svd = TruncatedSVD(n_components=20)
    svd.fit(X_train)

    test_preds = svd.inverse_transform(svd.transform(X_test))

    rmse_score = custom_rmse(X_test.values.flatten(), test_preds.flatten())
    mae_score = custom_mae(X_test.values.flatten(), test_preds.flatten())

    rmse_scores.append(rmse_score)
    mae_scores.append(mae_score)

print(f"CV RMSE: {np.mean(rmse_scores):.4f}")
print(f"CV MAE: {np.mean(mae_scores):.4f}")

"""Fungsi ini merekomendasikan 10 film teratas yang belum pernah ditonton oleh pengguna berdasarkan prediksi rating model."""

# Function to recommend movies for a specific user
def get_recommendations(user_id, movies_df, top_n=10):
    user_rated_movies = ratings[ratings['userId'] == user_id]['movieId'].values
    all_movie_ids = movies_df['movieId'].unique()
    unrated_movie_ids = np.setdiff1d(all_movie_ids, user_rated_movies)

    predictions = [model.predict(user_id, movie_id) for movie_id in unrated_movie_ids]
    predictions.sort(key=lambda x: x.est, reverse=True)

    recommended_movie_ids = [pred.iid for pred in predictions[:top_n]]
    recommendations = movies_df[movies_df['movieId'].isin(recommended_movie_ids)]
    return recommendations[['title', 'genres']]

"""Rekomendasi film untuk user id 331"""

def get_recommendations_svd(user_id, movies_df, ratings_df, top_n=10):
    # Buat pivot table
    ratings_pivot = ratings_df.pivot_table(
        index='userId',
        columns='movieId',
        values='rating',
        fill_value=0
    )

    # Latih model SVD
    svd = TruncatedSVD(n_components=20, random_state=42)
    svd.fit(ratings_pivot)

    # Dapatkan prediksi untuk semua film
    user_idx = ratings_pivot.index.get_loc(user_id)
    user_pred = svd.inverse_transform(svd.transform(ratings_pivot.iloc[user_idx:user_idx+1]))

    # Buat DataFrame prediksi
    pred_df = pd.DataFrame({
        'movieId': ratings_pivot.columns,
        'pred_rating': user_pred.flatten()
    })

    # Gabungkan dengan film yang sudah ditonton
    watched = ratings_df[ratings_df['userId'] == user_id][['movieId']]
    recommendations = pred_df[~pred_df['movieId'].isin(watched['movieId'])] \
        .sort_values('pred_rating', ascending=False) \
        .head(top_n)

    return recommendations.merge(movies_df, on='movieId')

# Contoh penggunaan
svd_recommendations = get_recommendations_svd(331, movies, ratings, top_n=10)
print(svd_recommendations)

"""Rekomendasi film untuk user id 21"""

# Contoh penggunaan
svd_recommendations = get_recommendations_svd(21, movies, ratings, top_n=10)
print(svd_recommendations)

"""Rekomendasi film untuk user id 1"""

# Contoh penggunaan
svd_recommendations = get_recommendations_svd(1, movies, ratings, top_n=10)
print(svd_recommendations)